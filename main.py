from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql.types import *
from pyspark.sql.functions import year, mean
import pandas as pd
import matplotlib.pyplot as plt
from py4j.java_gateway import java_import




# This function is renaming the files generated by the cube.
# Due to spark clustering the files, we have to use Hadoop to access the File System
def renameFile(type, spark): 
    # Determine the correct name to use.  
    if type == 1:
        name = 'none'
    elif type == 2:
        name = 'Department'
    elif type == 3:
        name = 'Gender'
    else:
        name = 'Gender_Department'

    # Set up hadoop filesystem.    
    java_import(spark._jvm, 'org.apache.hadoop.fs.Path')
    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())

    # Get the file name. I have prespecified the directory when exporting the data, and now i get all the files (1)
    # in that directory
    file = fs.globStatus(sc._jvm.Path('exported-data/part*'))[0].getPath().getName()
    # Rename the file to the desired name based on the input type and place it in a final directory
    fs.rename(sc._jvm.Path('exported-data/' + file), sc._jvm.Path('final_data/'+name+'.csv'))

    # Delete the old directory
    fs.delete(sc._jvm.Path('exported-data'), True)

# Initialize spark session
spark = SparkSession.builder.appName("BigDataProject").getOrCreate()

# Get the files
loansfile = "/opt/spark/LOANS.TXT"
borrsFile = '/opt/spark/BORROWERS.TXT'

sc = spark.sparkContext
spark.sparkContext.setLogLevel('WARN')

# Schema 1. The loans schema. This will be used to create the table
loansSchema = StructType([
   StructField("bibno", IntegerType(), True),
   StructField("copyno", StringType(), True),
   StructField("bid", StringType(), True),
   StructField("date_key", DateType(), True)])


print("Starting the data import: ")
# Load and place in a temp view the loans.
# The options are used to determine the correct date format, delimiter of txt file and that the file contains headers (to be removed)
loansDF = spark.read.schema(loansSchema).option('dateFormat','yyyy-mm-dd').option('delimiter','|').option('header', 'true').csv(loansfile)
loansDF.createOrReplaceTempView('tempLoans')

# Schema 2. The borrowers schema
borrowersSchema = StructType([
    StructField("bid", IntegerType(), True),
    StructField('gender', StringType(), True),
    StructField('department', StringType(), True)])

# Same with line 55
borrDF = spark.read.schema(borrowersSchema).option('headers', 'true').option('delimiter','|').csv(borrsFile)
borrDF.createOrReplaceTempView('tempBorrs')

print("Import finished. Proceeding with the questions: ")
# Join the tables on the common attribute
ij = loansDF.join(borrDF, loansDF.bid == borrDF.bid)


# Create the desired cube.
cres = ij.cube(ij["gender"], ij["department"]).count().orderBy("gender", "department")

# Now I need to create the desired files. The logic to extract the different group bys is as follows: 
# logic: 
# gbn: dept && gender isNull
# gbd: dept not null && gender null
# gbg: dept null && gender not null
# gbgd: dept not null && gender not null
# gbg = cres.filter()

# In the following code, I use repartition(1) in order to write only one file.
# If we emit this, then multiple files will be created.

# Then, based on the logic above I use filter on the cube results in order to get the different group by
# In the rename function I pass the mode we want to save and the spark instanc
print("Question 1:")
print("Starting calculating the seperate group bys and creating their files")

# Calculate group by none (total)
gbn = cres.filter(cres.gender.isNull() & cres.department.isNull())
gbn.repartition(1).write.csv('exported-data')
renameFile(1,spark)
print("File of group by none created")

# -----------------------------------------------------------------

# Calculate group by dept: 
gbd = cres.filter(cres.department.isNotNull() & cres.gender.isNull())
gbd.repartition(1).write.csv('exported-data')
renameFile(2,spark)
print("File of group by department created")

# -----------------------------------------------------------------

# Calculate group by gender
gbg = cres.filter(cres.department.isNull() & cres.gender.isNotNull())
gbg.repartition(1).write.csv('exported-data')
renameFile(3,spark)
print("File of group by gender created")

# -----------------------------------------------------------------
# Calculate group by gender, department
gbdg = cres.filter(cres.department.isNotNull() & cres.gender.isNotNull())
gbdg.repartition(1).write.csv('exported-data')
renameFile(4,spark)
print("File of group by gender, department created")

print("Done writing the files")

# Question 2 - Get the departments of which the female loans are greater than the male

# ----------This is working for q2------------------------

# I use the group by(department, gender) in order to reuse data.
# I perform the filtering as follows: 
# I get a df with only male.
# I get a df with only female.
# I join them on department after renaming the count of females to female_count to deferantiate the two counts.
# Then i create a temporary table, i perform the desired select and return the data.
print("Question 2:")
gbfe = gbdg.filter(gbdg.gender == 'F')
gbma = gbdg.filter(gbdg.gender == 'M')
joined = gbma.join(gbfe.withColumnRenamed('count', 'female_count'), ['department'])
joined.createOrReplaceTempView('deptGender')

data = spark.sql('SELECT department from deptGender where female_count > count')

print("Showing the departments the female students of which, made more loans than their male students")
data.show()
# ----------This is working for q2------------------------


# Question 3 - Create a plot showing the total loans per year.


# Calculate the total loans per year
# In the loans df i group by year. I make sure to rename the count, because in pandas, the count is a function
# and wont be passed in the plot (2 hours on this error)
print("Question 3:")
loansPerYear = loansDF.groupBy(year('date_key').alias('year')).count().withColumnRenamed("count","totals")

# Transform spark df to pandas df
df = loansPerYear.toPandas()

# Get the dimentions for the plot.
labels = df.year
sizes = df.totals

# Create the pie chart
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')
plt.title('Total Loans per Year')

print("Created the plot and exporting it: ")
# Show it
plt.show()
